{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688072a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install beautifulsoup4,                             installしたら消す\n",
    "# conda install requests\n",
    "# conda install selenium\n",
    "# conda install Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://~\"\n",
    "savefile = \"sample.png\"\n",
    "\n",
    "urllib.request.urlretrieve(url, savefile)\n",
    "print(\"保存しました\")\n",
    "\n",
    "# 一度pythonのメモリにデータを取得してから、ファイルに保存する。\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "with open(savefile, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e47800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列を変換\n",
    "url = \"https://~\"\n",
    "res = urllib.request.urlopen(url)\n",
    "data = res.read()\n",
    "\n",
    "test = data.decode(\"utf-8\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a619930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 住所検索\n",
    "import urllib.request as req\n",
    "import urllib.parse as parse\n",
    "\n",
    "API = \"https://api.aoikujira.com/zip/xml/get.php\"\n",
    "\n",
    "values = {'fml' : 'xml',  #'xml' or 'ini' or 'json'\n",
    "          'zn' : '1500042'} # 調べるもの、一般化のsys.argv[]はここ！\n",
    "params = parse.urlencode(values)\n",
    "\n",
    "url = API + \"?\" + params\n",
    "print(\"url = \", url)\n",
    "\n",
    "data = req.urlopen(url).read()\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)\n",
    "\n",
    "# 上３行の別の記述\n",
    "with req.urlopen(url) as r:\n",
    "    b = r.read()\n",
    "    data = b.decode('utf-8')\n",
    "    print(data)\n",
    "    \n",
    "# コマンドラインで一般化もできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idで検索\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1 id=\"title\">スクレイピングとは</h1>\n",
    "    <p id=\"body\">Webページから任意のデータを抽出すること</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title = soup.find(id = \"title\")\n",
    "body = soup.find(id = \"body\")\n",
    "\n",
    "print(\"#title =\" + title.string)\n",
    "print(\"#body =\" + body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のタグを一気に入手する\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"https://~~\">uta</a></li>\n",
    "    <li><a href=\"https://--\">oto</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)\n",
    "\n",
    "# uta > https://~~    oto > https://--   と出力される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a15426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlからデータを抽出\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://api.aoikujira.com/zip/xml/1500042\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "ken = soup.find(\"ken\").string\n",
    "shi = soup.find(\"shi\").string \n",
    "cho = soup.find(\"cho\").string\n",
    "print(ken, shi, cho)\n",
    "\n",
    "# urlopen()でURLを開いて、BeautifulSoupで抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file ---> index.html\n",
    "\"\"\"\n",
    "<html><body>\n",
    "  <div id=\"main-goods\" role=\"page\">\n",
    "    <h1>フルーツや野菜</h1>\n",
    "    <ul id=\"fr-list\">\n",
    "      <li class=\"red green\" data-lo=\"jp\">リンゴ</li>\n",
    "      <li class=\"purple\" data-lo=\"us\">ブドウ</li>\n",
    "      <li class=\"yellow\" data-lo=\"us\">レモン</li>\n",
    "      <li class=\"yellow\" data-lo=\"jp\">オレンジ</li>\n",
    "    </ul>\n",
    "    <ul id=\"ve-list\">\n",
    "      <li class=\"white green\" data-lo=\"jp\">ダイコン</li>\n",
    "      <li class=\"red green\" data-lo=\"us\">パプリカ</li>\n",
    "      <li class=\"black\" data-lo=\"jp\">ナス</li>\n",
    "      <li class=\"black\" data-lo=\"us\">アボカド</li>\n",
    "      <li class=\"white\" data-lo=\"cn\">レンコン</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fp = open(\"index.html\", encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "#アボカドを抜き出す\n",
    "#CSSセレクター\n",
    "print(soup.select_one(\"li:nth-of-type(8)\").string)\n",
    "print(soup.select_one(\"#ve-list > li:nth-of-type(4)\").string)\n",
    "print(soup.select(\"#ve-list > li[data-lo=\"us\"]\")[1].string)\n",
    "print(soup.select(\"#ve-list > li.black\")[1].string)\n",
    "#findメソッド\n",
    "cond = {\"data-lo\":\"us\", \"class\":\"black\"}\n",
    "print(soup.find(\"li\", cond).string)\n",
    "print(soup.find(id=\"ve-list\").find(\"li\", cond).string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906540fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規表現\n",
    "import re\n",
    "\n",
    "html = \"\"\"\n",
    "<ul>\n",
    "  <li><a href=\"hoge.html\">hoge</li>\n",
    "  <li><a href=\"https://exaple.com/fuga\">fuga*</li>\n",
    "  <li><a href=\"https://exaple.com/foo\">foo*</li>\n",
    "  <li><a href=\"http://exaple.com/aaa\">aaa</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "li = soup.find_all(href=re.compile(r\"^https://\"))\n",
    "for e in li: print(e.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相対パスから絶対パス\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print( urljoin(base, \"b.html\") )                       #--->   http://example.com/html/b.html\n",
    "print( urljoin(base, \"sub/c.html\") )                   #--->   http://example.com/html/sub/c.html\n",
    "print( urljoin(base, \"../img/hoge.png\") )              #--->   http://example.com/index.html\n",
    "print( urljoin(base, \"/hoge.html\") )                   #--->   http://example.com/hoge.html\n",
    "print( urljoin(base, \"http://kujirahand.com/wiki\") )   #--->   http://kujirahand.com/wiki\n",
    "print( urljoin(base, \"//uta.pw/shodou\") )              #--->   http://uta.pw/shodou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c3bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 丸ごとダウンロードプログラム\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from  os import makedirs\n",
    "import os.path, time, re\n",
    "\n",
    "proc_files = {}\n",
    "\n",
    "# HTML内にあるリンクを抽出する関数\n",
    "def enum_links(html, base):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel='stylesheet']\")   #CSS\n",
    "    links += soup.select(\"a[href]\")  #リンク\n",
    "    result = []\n",
    "    for a in links:\n",
    "        href = a.attrs['href']\n",
    "        url = urljoin(base, href)\n",
    "        result.append(url)\n",
    "    return result\n",
    "\n",
    "# ファイルをダウンロードする関数\n",
    "def download_file(url):\n",
    "    o = urlparse(url)\n",
    "    savepath = \"./\" + o.netloc + o.path\n",
    "    if re.search(r\"/$\", savepath):\n",
    "        savepath += \"index.html\"\n",
    "    savedir = os.path.dirname(savepath)\n",
    "    if os.path.exists(savepath): return savepath\n",
    "    if not os.path.exists(savedir):\n",
    "        print(\"mkdir=\", savedir)\n",
    "        makedirs(savedir)\n",
    "    try:\n",
    "        print(\"download=\", url)\n",
    "        urlretrieve(url, savepath)\n",
    "        time.sleep(1)\n",
    "        return savepath\n",
    "    except:\n",
    "        print(\"ダウンロード失敗：\", url)\n",
    "        return None\n",
    "\n",
    "# HTMLを解析してダウンロードする関数\n",
    "def analize_html(url, root_url):\n",
    "    savepath = download_file(url)\n",
    "    if savepath is None: return\n",
    "    if savepath in proc_files: return\n",
    "    proc_files[savepath] = True\n",
    "    print(\"analize_html=\", url)\n",
    "    html = open(savepath, \"r\", encoding=\"utf-8\").read()\n",
    "    links = enum_links(html, url)\n",
    "    for link_url in links:\n",
    "        if link_url.find(root_url) != 0:\n",
    "            if not re.search(r\".css$\", link_url): continue\n",
    "        if re.search(r\".(html|htm)$\", link_url):\n",
    "            analize_html(link_url, root_url)\n",
    "            continue\n",
    "        download_file(link_url)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://docs.python.jp/3.6/library/\"\n",
    "    analize_html(url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db51ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests モジュール\n",
    "r = requests.get(\"https://~\")   # get\n",
    "formdata = {\"username\" : \"yuuki\", \n",
    "            \"password\" : \"1234567890\"}\n",
    "d = requests.post(\"https://~\", data=formdata) # post   cf. session(), put(\"URL\"), delete(\"URL\"), head(\"URL\")\n",
    "\n",
    "print(r.text)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium  (今回は firefox を使う)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93753408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずはドライバーが必要\n",
    "# firefox をインストールしたら、https://github.com/mozilla/geckodriver/releases で検索してドライバーをダウンロード\n",
    "# 日本語対応させたい場合は、調べる\n",
    "\n",
    "from selenium.webdriver import Firefox, FirefoxOptions\n",
    "# ヘッドレスモード\n",
    "options = FirefoxOptions()\n",
    "options.add_argument('-headless')\n",
    "browser = Firefox(options=options)\n",
    "# ページにアクセス\n",
    "url = \"https://~\"\n",
    "browser.get(url)\n",
    "# (選択)スクショ保存をする\n",
    "browser.save_screenshot(\"scsho.png\")\n",
    "browser.quit()\n",
    "# (選択)テキストボックスに文字を入力 idはディベロッパーツールで調べる\n",
    "e = browser.find_element_by_id(\"user\")\n",
    "e.clear()\n",
    "e.send_keys(\"yuuki\")\n",
    "e = browser.find_element_by_id(\"password\")\n",
    "e.clear()\n",
    "e.send_keys(\"0123456789\")\n",
    "frm = browser.find_element_by_css_selector(\"#loginForm form\")\n",
    "frm.submit()   # フォームを送信\n",
    "# ページのロードまで待つのでsleep(10)くらいを追加しておく\n",
    "# class名 .islogin\n",
    "a = browser.find_element_by_css_selector(\".islogin a\")\n",
    "url_mypage = a.get_attribute(\"href\")  # mypage のURL\n",
    "# マイページ表示\n",
    "browser.get(url_my_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定期的な情報収集  macOS, Linux   -->  Ubuntu を使う\n",
    "# dataget.pyの中\n",
    "import urllib.request as req\n",
    "import datetime\n",
    "import json\n",
    "API = \"URL\"\n",
    "json_str = req.urlopen(API).read().decode(\"utf-8\")   # .textとかで試してみる\n",
    "data = json.loads(json_str)\n",
    "print(\"情報を取得しました\")\n",
    "t = datetime,date.today()\n",
    "fname = t.strfname(\"%Y-%m-%d\") + \".json\"\n",
    "with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_str)\n",
    "    \n",
    "# nano エディターをインストール\n",
    "# cron をつかうことで　上記のファイルを定期実行することができ、自動でデータを取得することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4aee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロジェクトの作成\n",
    "scrapy startproject soseki_list  # シェルに打つ\n",
    "\"\"\"\n",
    "以下のようなファイル群が生成\n",
    "soseki_list\n",
    "  scrapy.cfg   --- 設定ファイル\n",
    "  soseki_list  --- この下がPythonモジュール\n",
    "    __init__.py\n",
    "    items.py   --- アイテム定義\n",
    "    middlewares.py   --- ミドルウェア\n",
    "    pipelines.py   --- パイプライン\n",
    "    settings.py  --- 設定\n",
    "    spiders   --- spider を配置するディレクトリー\n",
    "      __init__.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bceec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soseki_list/spiders/soseki.py\n",
    "import scrapy\n",
    "\n",
    "class SosekiSpider(scrapy.Spider):\n",
    "    name = 'soseki'   # spider の名前\n",
    "    start_urls = ['https://www.aozora.gr.jp/index_pages/person148.html']  # 取得対象のウェブサイト\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('title')\n",
    "        print(title.extract())    # リスト型で取り出す\n",
    "        \n",
    "# scrapy crawl soseki --nolog  を実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soseki_list/spiders/soseki2.py\n",
    "import scrapy\n",
    "\n",
    "class Soseki2Spider(scrapy.Spider):\n",
    "    name = 'soseki2'\n",
    "    start_urls = ['https://www.aozora.gr.jp/index_pages/person148.html']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        li_list = response.css('ol > li a')\n",
    "        for a in li_list:\n",
    "            href = a.css('::attr(href)').extract_first()\n",
    "            text = a.css('::text').extract_first()\n",
    "            href2 = response.urljoin(href)  #  相対パスから絶対パスへの変換\n",
    "            yield{'text' : 'text', \n",
    "                   'url' : 'href2'}   # 結果はreturn でなくyieldで返す。\n",
    "\n",
    "# scrapy crawl soseki2 -o list.json\n",
    "# python3\n",
    "# >>> import json, pprint\n",
    "# >>> a = json.load(open(\"list.json\"))\n",
    "# >>> pprit.pprint(a)\n",
    "\n",
    "# Scrapy をシェルで実行\n",
    "scrapy shell\n",
    "[1] fetch('https://~~')\n",
    "[2] response.css('title::text').extract_first()\n",
    "[3] response.css('#tblIndex > tr td:nth-child(2) a::text').extract()\n",
    "[4] quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce259ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定データをダウンロードする\n",
    "# scrapy genspider soseki3 www.aozora.gr.jp\n",
    "\n",
    "# soseki_list/spiders/soseki3.py\n",
    "import scrapy, pprint\n",
    "class Soseki4Spider(scrapy.Spider):\n",
    "    name = 'soseki4'\n",
    "    allowed_domains = ['www.aozora.gr.jp']\n",
    "    start_urls = ['htts://ww.aozora.gr.jp/index_pages/person148.html']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        li_list = response.css('ol > li a')\n",
    "        for a in li_list:\n",
    "            href = a.css('::attr(href)').extract_first()\n",
    "            href2 = response.urljoin(href)\n",
    "            yield response.follow(href2, self.parse_card)\n",
    "            \n",
    "    def parse_card(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        alist = response.css('table.download tr td a')\n",
    "        for a in alist:\n",
    "            href = a.css('::attr(href)').extract_first()\n",
    "            href2 = response.urljoin(href)\n",
    "            if href2[-4:] != \".zip\": continue\n",
    "            req = scrapy.Request(href2, callback=self.parse_item)  #ダウンロードの指示\n",
    "            req.meta[\"title\"] = title\n",
    "            yield req\n",
    "            \n",
    "    def parse_item(self, response):\n",
    "        title = response.meta[\"title\"]\n",
    "        title = title.replace('図書カード：', '').strip()\n",
    "        fname = title + \".zip\"\n",
    "        with open(fname, \"wb\") as f:\n",
    "            f.write(response.body)\n",
    "            \n",
    "# scrapy crawl soseki4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy のミドルウェアに selenium を組み込む(Javascriptなどを読み込めるようになる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f033f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy startproject sakusibbs\n",
    "# sakusibbs/selenium_middleware.py\n",
    "from scarapy.http import HtmlResponse\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "driver = Firefox()\n",
    "\n",
    "def selenium_get(url): # 任意のURLを開く\n",
    "    driver.get(url)\n",
    "    \n",
    "def get_dom(query): # 読み込み完了まで待機\n",
    "    dom = WedDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, query)))\n",
    "    return dom\n",
    "\n",
    "def selenium_close():\n",
    "    driver.close()\n",
    "\n",
    "class SeleniumMiddleware(object):  # ミドルウェア本体\n",
    "    def process_request(self, request, spider): # リクエストをseleniumで処理する\n",
    "        driver.get(request.url)\n",
    "        return HtmlResponse(driver.current_url, \n",
    "                             body = driver.page_source,\n",
    "                             encoding = 'utf-8',\n",
    "                             request = request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfe042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy genspider getall uta.pw\n",
    "# sakusibbs/spiders/getall.py\n",
    "import scrapy, pprint\n",
    "from ..selenium_middleware import *\n",
    "\n",
    "USER = \"yuuki\"\n",
    "PASS = \"123456789\"\n",
    "\n",
    "class GetallSpider(scrapy.Spider):\n",
    "    name = 'getall'\n",
    "    #ミドルウェアを登録\n",
    "    custom_settings = {\n",
    "        \"DOWNLOADER_MIDDLEWARES\":{\n",
    "            \"sakusibbs.selenium_middleware.SeleniumMiddleware\" : 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #ログイン\n",
    "    def start_requests(self):\n",
    "        url = 'https://uta.pw/sakusibbs/users.php?action=login'\n",
    "        selenium_get(url)  #ページを開く\n",
    "        user = get_dom('#user') # 入力\n",
    "        user.send_keys(USER) \n",
    "        pw = get_dom('#pass')\n",
    "        pw.send_keys(PASS)\n",
    "        btn = get_dom('#loginForm input[type=submit]')  # ボタンをクリック\n",
    "        btn.click()\n",
    "        a = get_dom('.islogin a')  # ユーザーページを得る\n",
    "        mypage = a.get_getattribute('href')\n",
    "        print(\"mypage:\", mypage)\n",
    "        yield scrapy.Request(mypage, self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for a in list:\n",
    "            url = a.css('::attr(href)').extract_first()\n",
    "            url2 = response.urljoin(url)\n",
    "            yield response.follow(url2, self.parse_sakuhin)\n",
    "            \n",
    "    def parse_sakuhin(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        print(\"---\", title)\n",
    "        src = response.css('iframe::attr(src)').extract_first()\n",
    "        src2 = response.urljoin(src)\n",
    "        req = scrapy.Request(src2, self.parse_download)\n",
    "        req.meta[\"title\"] = title\n",
    "        yield req\n",
    "        \n",
    "    def parse_download(self, response):\n",
    "        title = response.meta[\"title\"]\n",
    "        fname = title + \".html\"\n",
    "        with open(fname, \"wt\") as f:\n",
    "            f.write(response.body)\n",
    "            \n",
    "    def closed(self, reason):\n",
    "        selenium_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web API\n",
    "# http://Webservice.rakuten.co.jp/document/\n",
    "# http://developer.yahoo.co.jp/Webapi/shopping/\n",
    "# オープンデータAPI\n",
    "# http://www.e-stat.go.jp/api/  政府統計の総合窓口\n",
    "# http://iss.ndl.go.jp/information/api/  国会国立図書館\n",
    "# https://www.hellowork.go.jp/info/online02.html  ハローワーク\n",
    "# http://www.land.mlit.go.jp/Webland/api.html  土地総合情報システム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 株価、為替情報のスクレイピング    -->   調べる(スクレイピングが禁止されていないか)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e577b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出は、開発者ツール(ディベロッパーツール)を使うか、ソースコードの表示を用いる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
